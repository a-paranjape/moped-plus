{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aff408-aed4-41f1-aded-b79340f0ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys,shutil,os\n",
    "from scipy import linalg\n",
    "import scipy.special as sysp\n",
    "from scipy import stats \n",
    "\n",
    "sys.path.append('../code/')\n",
    "from sbi import NeuralRatioEstimator\n",
    "\n",
    "import copy,pickle\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as pltcol\n",
    "import gc\n",
    "\n",
    "import psutil\n",
    "\n",
    "# cobaya imports\n",
    "from cobaya.run import run\n",
    "from cobaya.log import LoggedError\n",
    "from getdist.mcsamples import loadMCSamples\n",
    "import getdist.plots as gdplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ee917-7dff-4549-b634-914c8ae8a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['xtick.direction'] = 'in'\n",
    "mpl.rcParams['ytick.direction'] = 'in'\n",
    "mpl.rcParams['xtick.top'] = True\n",
    "mpl.rcParams['ytick.right'] = True\n",
    "mpl.rcParams['xtick.labelsize'] = 14\n",
    "mpl.rcParams['ytick.labelsize'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['legend.fontsize'] = 12 # 14\n",
    "mpl.rcParams['legend.labelspacing'] = 0.3\n",
    "FS = 18\n",
    "FS2 = 15\n",
    "FS3 = 13\n",
    "FSL = 22\n",
    "\n",
    "mpl.rcParams['xtick.major.size'] = 6\n",
    "mpl.rcParams['xtick.minor.size'] = 3\n",
    "mpl.rcParams['ytick.major.size'] = 6\n",
    "mpl.rcParams['ytick.minor.size'] = 3\n",
    "\n",
    "#mpl.rcParams.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba4f11-79a0-4174-b48d-644d2a4ff6e8",
   "metadata": {},
   "source": [
    "# Example usage of NeuralRatioEstimator\n",
    "### simple NRE using Sequential NN tested on (non-)linear Gaussian problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8130a605-fa19-4abf-aa29-be652a4ce4be",
   "metadata": {},
   "source": [
    "## Simulator setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56875477-3b1c-4a65-b591-4b352edafe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "nparam = 2\n",
    "ndata = 15\n",
    "\n",
    "NReal = 20\n",
    "\n",
    "File_Stem = 'poly' # 'poly','gaussmix'\n",
    "if (File_Stem == 'poly') & (nparam > 2):\n",
    "    File_Stem += '_deg{0:d}'.format(nparam-1)\n",
    "if NReal != 10:\n",
    "    File_Stem += '_avg{0:d}'.format(NReal)\n",
    "Plots_Dir = File_Stem + '/plots/'\n",
    "print('Using file stem:'+File_Stem)\n",
    "print('Using plot dir :'+Plots_Dir)\n",
    "\n",
    "class MyNRE(NeuralRatioEstimator):\n",
    "    def __init__(self,params={}):\n",
    "        NeuralRatioEstimator.__init__(self,params=params)\n",
    "\n",
    "        if self.file_stem[:8] == 'gaussmix':\n",
    "            if (self.nparam % 3) != 0:\n",
    "                raise ValueError(\"param_dim must be multiple of 3\")\n",
    "            self.ncomp = self.nparam // 3\n",
    "        \n",
    "        # data variables and noise\n",
    "        self.xvals = np.linspace(-1.5,3.5,self.ndata)\n",
    "        # self.sigma = 8*np.linspace(0.05,0.2,self.ndata)\n",
    "        self.sigma = np.linspace(0.1,0.5,ndata)\n",
    "        self.cov_mat = np.diagflat(self.sigma**2)\n",
    "\n",
    "        # prior mean,std\n",
    "        self.prior_mean = np.zeros(self.nparam)\n",
    "        self.prior_std = 6*np.ones(self.nparam)\n",
    "        self.prior_cov = np.diagflat(self.prior_std**2)\n",
    "\n",
    "        # # prior bounds\n",
    "        # self.theta_min = -10*np.ones(self.nparam)\n",
    "        # self.theta_max = 10*np.ones(self.nparam)\n",
    "        # self.dtheta = self.theta_max - self.theta_min        \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def simulator(self,theta):\n",
    "        out = np.zeros((self.ndata,theta.shape[1]))\n",
    "        if self.file_stem[:4] == 'poly':\n",
    "            ####################\n",
    "            # polynomial\n",
    "            out += np.sum(np.array([np.outer(self.xvals**p,theta[p]) for p in range(self.nparam)]),axis=0)\n",
    "            # for x in range(self.xvals.size):\n",
    "            #     out[x] = np.sum([theta[p]*self.xvals[x]**p for p in range(self.nparam)])\n",
    "            ####################\n",
    "        else:\n",
    "            for c in range(self.ncomp):\n",
    "                amp = theta[c*3]\n",
    "                mu = theta[c*3+1]\n",
    "                mu = np.outer(np.ones(self.ndata),mu)\n",
    "                lnsig2 = theta[c*3+2]\n",
    "                # amp,mu,lnsig2 = theta[c*3:(c+1)*3,:]\n",
    "                out += amp*np.exp(-0.5*(self.xvals-mu.T).T**2/np.exp(lnsig2))\n",
    "\n",
    "        noise = self.rng.multivariate_normal(np.zeros(self.ndata),self.cov_mat,size=out.shape[1]) # shape (out.shape[1],out.shape[0])\n",
    "        out += noise.T\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def prior(self,nsamp):\n",
    "        theta = np.zeros((self.nparam,nsamp))\n",
    "        \n",
    "        for p in range(self.nparam):\n",
    "            theta[p] = self.prior_mean[p] + self.rng.randn(nsamp)*self.prior_std[p]\n",
    "            \n",
    "        # for p in range(self.nparam):\n",
    "        #     theta[p] = self.theta_min[p] + self.rng.rand(nsamp)*self.dtheta[p]\n",
    "        \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d5dbf-8110-4469-a0f0-432f3871f20a",
   "metadata": {},
   "source": [
    "## Analytical ratio for linear Gaussian problem\n",
    "#### with design matrix $\\mathcal{M}$, data $y$, data covariance $C$, prior inverse covariance $F_{\\rm (p)}$, prior mean $\\theta_{\\rm (p)}$, we have\n",
    "#### $F = \\mathcal{M}^{\\rm T}C^{-1}\\mathcal{M} + F_{\\rm (p)}$\n",
    "#### $\\hat\\theta = F^{-1}\\left(\\mathcal{M}^{\\rm T}C^{-1}y + F_{\\rm (p)}\\theta_{\\rm (p)}\\right)$\n",
    "#### $-2\\ln p(\\theta|x) = (\\theta-\\hat\\theta)^{\\rm T}\\,F\\,(\\theta-\\hat\\theta) - \\ln{\\rm det}F + M\\ln(2\\pi)$\n",
    "#### $-2\\ln p(\\theta) = (\\theta-\\theta_{\\rm (p)})^{\\rm T}\\,F_{\\rm (p)}\\,(\\theta-\\theta_{\\rm (p)}) - \\ln{\\rm det}F_{\\rm (p)} + M\\ln(2\\pi)$\n",
    "#### $2\\ln r(y,\\theta) = 2\\ln\\left[p(\\theta|y)/p(\\theta)\\right] = (\\mathcal{M}\\theta)^{\\rm T}C^{-1}(y-\\mathcal{M}\\theta) + \\left(\\mathcal{M}(\\theta-\\hat\\theta)\\right)^{\\rm T}C^{-1}y - (\\hat\\theta-\\theta_{\\rm (p)})^{\\rm T}F_{\\rm (p)}\\theta_{\\rm (p)} + \\ln\\,{\\rm det}(FF_{\\rm (p)}^{-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326d80c-a5b2-43f1-bd99-c0b2d9d112a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_nre(nre,X,theta):\n",
    "    designM = np.ones((nre.ndata,nre.nparam))\n",
    "    for p in range(nre.nparam):\n",
    "        designM[:,p] = nre.xvals**p # monomial basis\n",
    "    F_p = np.diagflat(1/nre.prior_std**2) # prior inv Cov\n",
    "    theta_p = nre.cv(nre.prior_mean) # prior mean\n",
    "    Cinv,detC = nre.svd_inv(nre.cov_mat) #np.diagflat(1/nre.sigma**2) # data inv Cov\n",
    "    F = F_p + np.dot(designM.T,np.dot(Cinv,designM)) # posterior inv Cov\n",
    "    theta_hat = np.dot(F_p,theta_p) # posterior mean\n",
    "    theta_hat += np.dot(designM.T,np.dot(Cinv,X))\n",
    "    theta_hat = np.dot(linalg.inv(F),theta_hat)\n",
    "\n",
    "    Mtheta = np.dot(designM,theta)\n",
    "    Mtheta_hat = np.dot(designM,theta_hat)\n",
    "\n",
    "    lnr = np.dot(Mtheta.T,np.dot(Cinv,X-Mtheta))\n",
    "    lnr += np.dot((Mtheta-Mtheta_hat).T,np.dot(Cinv,X))\n",
    "    lnr -= np.dot((theta_hat-theta_p).T,np.dot(F_p,theta_p))\n",
    "    lnr += np.log(linalg.det(F)) - np.log(linalg.det(F_p))\n",
    "    lnr *= 0.5\n",
    "    ratio = np.exp(lnr)    \n",
    "    return ratio,F,theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b76ac1-4cf8-4683-a4e4-902734a1ae95",
   "metadata": {},
   "source": [
    "## NRE setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea77ddc-634a-43bf-a4b7-5d6ded8c0e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train = True\n",
    "\n",
    "ValFrac = 0.2\n",
    "\n",
    "start_time = time()\n",
    "if Train:\n",
    "    Parallel = True\n",
    "    \n",
    "    L1 = 1 # 1 # number of layers with first activation\n",
    "    AType1 = 'tanh' # 'tanh'\n",
    "    NNodes1 = 500 # 10,30,50,100,150,300,500\n",
    "    L2 = 1 # 1 # number of layers with second activation\n",
    "    AType2 = 'tanh' # 'tanh'\n",
    "    NNodes2 = 350 # 7,20,35,70,100,200,350\n",
    "    L3 = 0 # 1 # number of layers with third activation\n",
    "    AType3 = 'lrelu' # 'tanh'\n",
    "    NNodes3 = 0 # 20\n",
    "        \n",
    "    Share_Train_Data = True # True is more efficient (as expected) as well as higher quality (for some reason)\n",
    "    \n",
    "    Standardize = True\n",
    "    Weight_Decay = 0.0 # 0.0\n",
    "    LReLU_Slope = 1.5e-2 # 1e-2\n",
    "    \n",
    "    params = {'param_dim':nparam,'data_dim':ndata,'standardize':Standardize,'lrelu_slope':LReLU_Slope,\n",
    "              'Lh':L1+L2+L3,'n_hidden_layer':[NNodes1]*L1+[NNodes2]*L2+[NNodes3]*L3,'hidden_atypes':[AType1]*L1+[AType2]*L2+[AType3]*L3,\n",
    "              'wt_decay':Weight_Decay,'file_stem':File_Stem,'nreal':NReal,'parallel':Parallel,'share_train_data':Share_Train_Data}\n",
    "    \n",
    "    print('Setup...')\n",
    "    nre_setup = MyNRE(params=params)\n",
    "    NFree = nre_setup.net[1].calc_N_freeparams()\n",
    "\n",
    "    Fac = 0.8 # 0.8,1,2,4\n",
    "    NSamp = int(Fac*NFree/(1-ValFrac)) # 1*NFree \n",
    "    # NSamp = 50000 # 100k,50k    \n",
    "    Complexity = NFree*NSamp\n",
    "    # NSamp = 50000 # 640000 # ensure multiple of 100\n",
    "    # Complexity = NFree*NSamp//100\n",
    "    del nre_setup\n",
    "    print('\\nNFree = {0:d}; NSamp = {1:d}\\n'.format(NFree,NSamp))\n",
    "    \n",
    "    if NReal > 1:\n",
    "        for r in range(1,NReal+1):\n",
    "            Path(File_Stem+'/r{0:d}'.format(r)).rmdir()\n",
    "\n",
    "    File_Stem_This = File_Stem + '/C{0:d}'.format(Complexity)\n",
    "    params['file_stem'] = File_Stem_This\n",
    "    print('... setup complete with file stem:',params['file_stem'])\n",
    "        \n",
    "    if Train:\n",
    "        nre = MyNRE(params=params)\n",
    "        params_train = {'max_epoch':20000, # 16000\n",
    "                        'lrate':3e-5, # 3e-5\n",
    "                        'check_after':5000, # 1000\n",
    "                        'mb_count':int(np.sqrt((1-ValFrac)*NSamp)),'val_frac':ValFrac}\n",
    "        nre.train(NSamp,params=params_train)\n",
    "        nre.save()\n",
    "        nre.save_train(params_train)\n",
    "else:\n",
    "    #######################\n",
    "    # NReal = 20 (multi KL)\n",
    "    # progressive arch, proportional train size (train=0.8*free): 320445801,4329771601,34114459401\n",
    "    #######################\n",
    "    # NReal = 10 (multi KL)\n",
    "    # progressive arch, proportional train size (train=4*free): 351125,6973805,37019205,399707405,1602229005,21648858005\n",
    "    # progressive arch, proportional train size (train=2*free): 175430,3486312,18508242,199849232,801105552,10824396102\n",
    "    # progressive arch, proportional train size (train=free): 87715,1743156,9254121,99924616,400552776,5412198051\n",
    "    # progressive arch, proportional train size (train=0.8*free): 70225,194481,1394761,7403841,27050401,79941481,320445801,\n",
    "    #                                                             4329771601,34114459401\n",
    "    # fixed arch (50,35), progressive train size: 5442000,9251400,13605000,27210000\n",
    "    # fixed arch (100,70), progressive train size: 17882000,44705000,100139200,178820000\n",
    "    # fixed arch (500,350), progressive train size: 9235050000,18470100000\n",
    "    #######################\n",
    "    # NReal = 5\n",
    "    # 483,723,2410,4410,27210,52010,81630,89410,156030,268230,272100,\n",
    "    # 846100,894100,1200100,1790100,2682300,8941000\n",
    "    #######################\n",
    "    Complexity = 34114459401\n",
    "    File_Stem_This = File_Stem + '/C{0:d}'.format(Complexity)\n",
    "\n",
    "# done like this to properly update saved loss histories in parallel calculation\n",
    "with open(File_Stem_This + '/params.pkl', 'rb') as f:\n",
    "    params = pickle.load(f)  \n",
    "nre = MyNRE(params=params)\n",
    "nre.load()\n",
    "params_train = nre.load_train()\n",
    "    \n",
    "print('params_train:',params_train)\n",
    "nre.time_this(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d249e2-e8b2-48df-8830-6de30acbe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min val loss...')\n",
    "plt.yscale('log')\n",
    "for r in range(1,NReal+1):\n",
    "    min_val_loss = nre.net[r].val_loss[nre.net[r].val_loss > 0.0].min()\n",
    "    print('... r{0:d} = {1:.3e}'.format(r,min_val_loss))\n",
    "    plt.plot(nre.net[r].epochs,nre.net[r].training_loss,'k-',lw=0.5)\n",
    "    plt.plot(nre.net[r].epochs,nre.net[r].val_loss,'r-',lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ba180-b702-4343-a11b-cca0a011872c",
   "metadata": {},
   "source": [
    "## MCMC-based comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae61e55-7a1f-437a-be4a-bc409a7b5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_repeat = 50\n",
    "NProc = np.min([4,N_repeat]) \n",
    "\n",
    "print('Architecture:'+nre.file_stem+' ...')\n",
    "\n",
    "Like_Dir = '../code/likes/'\n",
    "\n",
    "Max_Samples = 1000000\n",
    "Rminus1_Stop = 0.01 # 0.005\n",
    "Rminus1_CL_Stop = 0.025 # 0.025\n",
    "Rminus1_CL_Level = 0.95 # 95\n",
    "\n",
    "Burn_In = 0\n",
    "Burn_Frac = 0.3 # for later use\n",
    "\n",
    "Latex_List = ['a_{{{0:d}}}'.format(p) for p in range(nre.nparam)]\n",
    "Params_List = ['a{0:d}'.format(p) for p in range(nre.nparam)]\n",
    "\n",
    "# Nd_ln_2pi = nre.ndata*np.log(2*np.pi)\n",
    "# Cinv,detC = nre.svd_inv(nre.cov_mat)\n",
    "# ln_det_Cd = np.log(detC)\n",
    "# constant = -0.5*(Nd_ln_2pi + ln_det_Cd) \n",
    "\n",
    "\n",
    "info_nre = {}\n",
    "info_nre['params'] = {}\n",
    "\n",
    "info_nre['likelihood'] = {'likelihoods.NRELike':\n",
    "                          {'python_path':Like_Dir}}\n",
    "\n",
    "for p in range(len(Params_List)):\n",
    "    ref = 0.0\n",
    "    info_nre['params'][Params_List[p]] = {'ref':{'min':ref-0.001,'max':ref+0.001},\n",
    "                                          'prior':{'dist':'norm','loc':nre.prior_mean[p],'scale':nre.prior_std[p]},\n",
    "                                          'proposal':0.01,'latex':Latex_List[p]}\n",
    "\n",
    "info_nre['sampler'] = {'mcmc':\n",
    "                       {'learn_proposal': True,\n",
    "                        'Rminus1_single_split': 4,\n",
    "                        'measure_speeds': True,\n",
    "                        'max_samples': Max_Samples,\n",
    "                        'max_tries': 1000,\n",
    "                        'Rminus1_stop': Rminus1_Stop,\n",
    "                        'Rminus1_cl_stop': Rminus1_CL_Stop,\n",
    "                        'Rminus1_cl_level': Rminus1_CL_Level,\n",
    "                        'burn_in': Burn_In}}\n",
    "info_output = File_Stem + '/stats/chains/'\n",
    "chain_plot_path = info_output+'/for_plots'\n",
    "Path(chain_plot_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "info_nre[\"force\"] = True    \n",
    "info_nre[\"debug\"] = 40 # 40 = only error output generated (https://docs.python.org/2/library/logging.html#logging-levels)\n",
    "\n",
    "def run_chains(nre,info_nre,is_nre):\n",
    "    updated_info_nre, sampler_nre = run(info_nre)\n",
    "        \n",
    "    # Load chains\n",
    "    input_path = str(Path(info_nre[\"output\"]).resolve()) # note info_nre\n",
    "    gd_sample = loadMCSamples(input_path,settings={'ignore_rows':Burn_Frac})\n",
    "    sample = gd_sample.samples.T\n",
    "    \n",
    "    # Subsample chains\n",
    "    N_Boot = np.min([10000,int(0.2*sample[0].size)])\n",
    "    Ind = gd_sample.random_single_samples_indices(random_state=42,max_samples=N_Boot)\n",
    "    N_Boot = Ind.size\n",
    "    \n",
    "    # KL divergence D_{KL}(post_NRE || post_MCMC)\n",
    "    data_this = (info_nre['theory']['likelihoods.NRETheory']['data'] \n",
    "                 if is_nre else \n",
    "                 info_nre['likelihood']['likelihoods.Chi2Like']['Y'].T)\n",
    "    KL = 0.0\n",
    "    for b in range(N_Boot):\n",
    "        # lnPrior_b = -sample[-4,Ind[b]]\n",
    "        theta_b = nre.cv(sample[:nre.nparam,Ind[b]])\n",
    "        ratio,F,theta_hat = analytical_nre(nre,data_this,theta_b)\n",
    "        ratio_NRE = nre.predict(data_this,theta_b)\n",
    "        KL += np.log((ratio_NRE+1e-30)/(ratio+1e-30))\n",
    "        # lnPostNRE_b = np.log(nre.predict(data_this,theta_b)) + lnPrior_b\n",
    "        # # chi2_b = sample[-2,Ind[b]]\n",
    "        # # lnPostMCMC_b = -0.5*chi2_b + lnPrior_b + constant\n",
    "        # lnPostMCMC_b = np.log(ratio) + lnPrior_b\n",
    "        # KL += lnPostMCMC_b - lnPostNRE_b\n",
    "    KL /= (N_Boot*np.log(2))\n",
    "    return KL\n",
    "\n",
    "Tasks = []\n",
    "print('... setting up tasks')\n",
    "for r in range(N_repeat):\n",
    "    theta_test_this = nre.prior(1)\n",
    "    data_test_this = nre.simulator(theta_test_this)\n",
    "\n",
    "    info_nre_r = copy.deepcopy(info_nre)\n",
    "    nre_r = copy.deepcopy(nre)\n",
    "    \n",
    "    info_nre_r['output'] = info_output + 'nre_C{0:d}_r{1:d}'.format(Complexity,r+1)    \n",
    "    info_nre_r['theory'] = {'likelihoods.NRETheory':\n",
    "                            {'python_path':Like_Dir,'nre':nre_r,\n",
    "                             'data':data_test_this,\n",
    "                             'keys':Params_List}}\n",
    "    \n",
    "    if r == 0:\n",
    "        Tasks.append((nre_r,info_nre_r,False,run_chains))\n",
    "    Tasks.append((nre_r,info_nre_r,True,run_chains))\n",
    "\n",
    "    del nre_r,info_nre_r\n",
    "    gc.collect()\n",
    "    \n",
    "    nre.status_bar(r,N_repeat)\n",
    "\n",
    "def queue_chains(r,nre,info_nre,is_nre,run_chains,mdict):\n",
    "    # p = psutil.Process()\n",
    "    # p.cpu_affinity([n])\n",
    "    if not is_nre:\n",
    "        info_this = copy.deepcopy(info_nre)\n",
    "        data_this = info_this['theory']['likelihoods.NRETheory']['data'].copy()\n",
    "        info_this['output'] = info_output + 'mcmc_C{0:d}_r{1:d}'.format(Complexity,r+1)\n",
    "        info_this['likelihood'] = {'likelihoods.Chi2Like':\n",
    "                                  {'python_path':Like_Dir,\n",
    "                                   'X':nre.rv(nre.xvals),'Y':data_this.T,\n",
    "                                   'cov_mat':nre.cov_mat}}\n",
    "        if nre.file_stem[:8] == 'gaussmix':\n",
    "            info_this['theory'] = {'examplelikes.GaussMixTheory':\n",
    "                                  {'python_path':Like_Dir,\n",
    "                                   'X':nre.rv(nre.xvals),'ncomp':nre.ncomp}}\n",
    "        else:\n",
    "            info_this['theory'] = {'examplelikes.PolyTheory':\n",
    "                                  {'python_path':Like_Dir,\n",
    "                                   'X':nre.rv(nre.xvals)}}\n",
    "        del data_this\n",
    "        gc.collect()\n",
    "    else:\n",
    "        info_this = info_nre\n",
    "    KL = run_chains(nre,info_this,is_nre)\n",
    "    mdict[r+1] = KL\n",
    "    # mdict[r+1] = {'KL':KL,'data':info_nre['likelihoods.NRETheory']['data']}\n",
    "    return\n",
    "\n",
    "# from queuer import queue_chains\n",
    "\n",
    "KLvals = np.zeros(N_repeat)\n",
    "# Data = np.zeros((N_repeat,nre.ndata,1))\n",
    "start_time = time()\n",
    "KLDict = nre.run_processes(Tasks,queue_chains,NProc)\n",
    "for r in range(N_repeat):\n",
    "    KLvals[r] = KLDict[r+1]#['KL']\n",
    "    # Data[r] = KLDict[r+1]['data']\n",
    "    \n",
    "print('... cleanup')\n",
    "\n",
    "files = [f for f in Path().glob(info_output+'mcmc_C{0:d}_r1.*'.format(Complexity))]\n",
    "for f in files:\n",
    "    shutil.copy(f,chain_plot_path)\n",
    "\n",
    "files = [f for f in Path().glob(info_output+'nre_C{0:d}_r1.*'.format(Complexity))]\n",
    "for f in files:\n",
    "    shutil.copy(f,chain_plot_path)\n",
    "\n",
    "files = [f for f in Path().glob(info_output+'mcmc_C*.*')]\n",
    "for f in files:\n",
    "    Path.unlink(f)\n",
    "\n",
    "files = [f for f in Path().glob(info_output+'nre_C*.*')]\n",
    "for f in files:\n",
    "    Path.unlink(f)\n",
    "\n",
    "del Tasks\n",
    "gc.collect()\n",
    "\n",
    "nre.time_this(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1534a1-736c-468d-836d-8e0fba2b1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_finite = np.where(np.isfinite(KLvals))[0]\n",
    "KL = np.median(KLvals[ind_finite])\n",
    "KL84 = np.percentile(KLvals[ind_finite],84)\n",
    "KL16 = np.percentile(KLvals[ind_finite],16)\n",
    "KLstd = 0.5*(KL84 - KL16)\n",
    "print('Used {0:d} of {1:d} measured values'.format(ind_finite.size,KLvals.size))\n",
    "print('KL + lg(evidence) = {0:.4f} +- {1:.4f}\\n'.format(KL,KLstd))\n",
    "\n",
    "First_Arch = False\n",
    "\n",
    "Arch_File = File_Stem + '/archs.pkl'\n",
    "if First_Arch:\n",
    "    archs = {Complexity:{'KL':KL,'KL84':KL84,'KL16':KL16,'KLstd':KLstd,'Nwts':nre.net[1].calc_N_freeparams()}}\n",
    "else:\n",
    "    print('Reading architecture data from:',Arch_File)\n",
    "    with open(Arch_File, 'rb') as f:\n",
    "        archs = pickle.load(f)  \n",
    "    archs[Complexity] = {'KL':KL,'KL84':KL84,'KL16':KL16,'KLstd':KLstd,'Nwts':nre.net[1].calc_N_freeparams()}\n",
    "\n",
    "print('Storing architecture data to:',Arch_File)\n",
    "with open(Arch_File, 'wb') as f:\n",
    "    pickle.dump(archs,f)  \n",
    "archs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a56bd-67eb-4777-85d9-c19c11851523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arch_File = File_Stem + '/archs.pkl'\n",
    "# with open(Arch_File, 'rb') as f:\n",
    "#     archs = pickle.load(f)  \n",
    "# archs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd1538-9ac3-4727-9d8f-830b7092e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arch_File = File_Stem + '/archs.pkl'\n",
    "with open(Arch_File, 'rb') as f:\n",
    "    archs = pickle.load(f)  \n",
    "Complexities = list(archs.keys())#[:-1] \n",
    "N_Archs = len(Complexities)\n",
    "# storage for median,+error,-error,complexity,Nfree,Ntrain/Nfree\n",
    "KLstats = np.zeros(N_Archs,dtype=[('KL',float),('KL+',float),('KL-',float),\n",
    "                                  ('complexity',int),('Nwts',int),('freedom',float)]) \n",
    "for a in range(N_Archs):\n",
    "    complexity = Complexities[a]\n",
    "    KLstats['KL'][a] = archs[complexity]['KL']\n",
    "    KLstats['KL+'][a] = archs[complexity]['KL84'] - archs[complexity]['KL']\n",
    "    KLstats['KL-'][a] = archs[complexity]['KL'] - archs[complexity]['KL16']\n",
    "    KLstats['complexity'][a] = complexity\n",
    "    KLstats['Nwts'][a] = archs[complexity]['Nwts']\n",
    "    KLstats['freedom'][a] = (1-ValFrac)*complexity/archs[complexity]['Nwts']**2\n",
    "\n",
    "KLstats = np.sort(KLstats,order='complexity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8ae2a-779f-4cae-a5c4-1f5b6ea91f32",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0ef72-018e-4f4c-8cce-24c0c11e78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Save_Fig = True\n",
    "\n",
    "Show_MCMC = True\n",
    "Show_NRE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307441d5-2b8b-4c33-b846-fe4874bbb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.xlim(2e4,2e11)\n",
    "plt.ylim(8e-2,2e2)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "eps = 1e-1\n",
    "# c8941 = (KLstats['Nwts'] == 8941)\n",
    "cfree = (KLstats['freedom'] > 1+eps) #& ~c8941\n",
    "plt.errorbar(KLstats['complexity'][cfree],\n",
    "             KLstats['KL'][cfree],\n",
    "             yerr=np.array([KLstats['KL-'][cfree],KLstats['KL+'][cfree]]),\n",
    "             c='crimson',marker='o',capsize=5,label='free',lw=1)\n",
    "cmarg = (np.fabs(KLstats['freedom'] - 1.0) <= eps) #& ~c8941\n",
    "plt.errorbar(KLstats['complexity'][cmarg],\n",
    "             KLstats['KL'][cmarg],\n",
    "             yerr=np.array([KLstats['KL-'][cmarg],KLstats['KL+'][cmarg]]),\n",
    "             c='indigo',marker='o',capsize=5,label='marginal',lw=1)\n",
    "ccons = (KLstats['freedom'] < 1-eps) #& ~c8941\n",
    "plt.errorbar(KLstats['complexity'][ccons],\n",
    "             KLstats['KL'][ccons],\n",
    "             yerr=np.array([KLstats['KL-'][ccons],KLstats['KL+'][ccons]]),\n",
    "             c='gray',marker='o',capsize=5,label='constrained',lw=1)\n",
    "# plt.errorbar(KLstats['complexity'][c8941],\n",
    "#              KLstats['KL'][c8941],\n",
    "#              yerr=np.array([KLstats['KL-'][c8941],KLstats['KL+'][c8941]]),\n",
    "#              c='k',marker='o',capsize=5,label='Nwts = 8941',lw=1,alpha=0.1)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa2de5-2b86-417d-835f-4a5fb4413028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1cf98-a79e-4f53-921a-84586c923c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "n_params = len(nre.prior_mean)\n",
    "dim = 1*n_params\n",
    "dof = ndata - n_params\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# gd_sample = sampler.products(to_getdist=True)[\"sample\"]\n",
    "\n",
    "if Show_MCMC:\n",
    "    gd_sample = loadMCSamples(str(Path(chain_plot_path+'/mcmc_C{0:d}_r1'.format(Complexity)).resolve()),\n",
    "                                  settings={'ignore_rows':Burn_Frac})\n",
    "    gd_sample.label = 'MCMC' \n",
    "    # samples contain params | chi2 | chi2__name | ?? | ??\n",
    "    mcmc_covmat = gd_sample.getCovMat().matrix[:dim, :dim]\n",
    "    sample = gd_sample.samples\n",
    "    sample = sample.T\n",
    "    ibest = sample[-2].argmin()\n",
    "    mcmc_best = sample[:dim,ibest]\n",
    "    mcmc_chi2 = sample[-2,ibest]\n",
    "    pval = sysp.gammainc(mcmc_chi2/2,dof/2)\n",
    "    mcmc_sig = np.sqrt(np.diag(mcmc_covmat))\n",
    "    print('MCMC...')\n",
    "    print(\"... best fit ( a0,...a{0:d}) = ( \".format(n_params-1)+','.join(['%.4e' % (pval,) for pval in mcmc_best])+\" )\")\n",
    "    print(\"... std dev  ( a0,...a{0:d}) = ( \".format(n_params-1)+','.join(['%.4e' % (pval,) for pval in mcmc_sig])+\" )\")\n",
    "    print(\"... chi2_best,dof,chi2_red,pval: {0:.3f},{1:d},{2:.3f},{3:.3e}\".format(mcmc_chi2,dof,mcmc_chi2/dof,pval))\n",
    "\n",
    "if Show_NRE:\n",
    "    gd_sample_nre = loadMCSamples(str(Path(chain_plot_path+'/nre_C{0:d}_r1'.format(Complexity)).resolve()),\n",
    "                                  settings={'ignore_rows':Burn_Frac})\n",
    "    gd_sample_nre.label = 'NRE: C{0:d}'.format(Complexity)\n",
    "    # samples contain params | chi2 | chi2__name | ?? | ??\n",
    "    mcmc_covmat_nre = gd_sample_nre.getCovMat().matrix[:dim, :dim]\n",
    "    sample_nre = gd_sample_nre.samples\n",
    "    sample_nre = sample_nre.T\n",
    "    iNRE_Id = sample_nre[-2].argmin()\n",
    "    mcmc_NRE_Id = sample_nre[:dim,iNRE_Id]\n",
    "    mcmc_chi2_nre = sample_nre[-2,iNRE_Id]\n",
    "    pval_nre = sysp.gammainc(mcmc_chi2_nre/2,dof/2)\n",
    "    mcmc_sig_nre = np.sqrt(np.diag(mcmc_covmat_nre))\n",
    "    print('NN...')\n",
    "    print(\"... best fit ( a0,...a{0:d}) = ( \".format(n_params-1)+','.join(['%.4e' % (pval,) for pval in mcmc_NRE_Id])+\" )\")\n",
    "    print(\"... std dev  ( a0,...a{0:d}) = ( \".format(n_params-1)+','.join(['%.4e' % (pval,) for pval in mcmc_sig_nre])+\" )\")\n",
    "    print(\"... chi2_best,dof,chi2_red,pval: {0:.3f},{1:d},{2:.3f},{3:.3e}\".format(mcmc_chi2_nre,dof,mcmc_chi2_nre/dof,pval_nre))\n",
    "\n",
    "plot_param_list = Params_List\n",
    "Subplot_Size = 1.6\n",
    "\n",
    "gdplot = gdplt.get_subplot_plotter(subplot_size=Subplot_Size)\n",
    "gdplot.settings.num_plot_contours = 3\n",
    "gdplot.settings.axes_fontsize = FS3\n",
    "gdplot.settings.axes_labelsize = FS2\n",
    "gdplot.settings.title_limit_fontsize = FS3\n",
    "\n",
    "show_list = []\n",
    "fill_list = []\n",
    "col_list = []\n",
    "if Show_MCMC:\n",
    "    show_list.append(gd_sample)\n",
    "    fill_list.append(True)\n",
    "    col_list.append('indigo')\n",
    "if Show_NRE:\n",
    "    show_list.append(gd_sample_nre)\n",
    "    fill_list.append(True)\n",
    "    col_list.append('crimson')\n",
    "gdplot.triangle_plot(show_list, plot_param_list,filled=fill_list,\n",
    "                     contour_colors=col_list,legend_loc='upper right',\n",
    "                     title_limit=0)\n",
    "for par_y in range(dim):\n",
    "    str_y = plot_param_list[par_y]\n",
    "    ax = gdplot.subplots[par_y,par_y]\n",
    "    if Show_MCMC:\n",
    "        ax.axvline(mcmc_best[par_y],c='indigo',ls='--',lw=1,alpha=0.6)\n",
    "    if Show_NRE:\n",
    "        ax.axvline(mcmc_NRE_Id[par_y],c='crimson',ls='--',lw=1,alpha=0.6)\n",
    "    for par_x in range(par_y):\n",
    "        str_x = plot_param_list[par_x]\n",
    "        ax = gdplot.subplots[par_y,par_x]\n",
    "        if Show_MCMC:\n",
    "            ax.scatter([mcmc_best[par_x]],[mcmc_best[par_y]],marker='*',s=50,c='aliceblue')\n",
    "            ax.axvline(mcmc_best[par_x],c='indigo',ls='--',lw=1,alpha=0.6)\n",
    "            ax.axhline(mcmc_best[par_y],c='indigo',ls='--',lw=1.5,alpha=0.6)\n",
    "        if Show_NRE:\n",
    "            ax.scatter([mcmc_NRE_Id[par_x]],[mcmc_NRE_Id[par_y]],marker='*',s=50,c='peachpuff')\n",
    "            ax.axvline(mcmc_NRE_Id[par_x],c='crimson',ls='--',lw=1,alpha=0.6)\n",
    "            ax.axhline(mcmc_NRE_Id[par_y],c='crimson',ls='--',lw=1,alpha=0.6)\n",
    "\n",
    "if Save_Fig:\n",
    "    filename = 'contours_'+File_Stem+'_C{0:d}.png'.format(Complexity)\n",
    "    print('Writing to file: '+Plots_Dir+filename)\n",
    "    gdplot.export(fname=filename,adir=Plots_Dir)\n",
    "\n",
    "nre.time_this(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43faf3a-a321-48d4-a3f0-a6800a168f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_repeat = 50\n",
    "# NProc = np.min([4,N_repeat]) \n",
    "\n",
    "# print('Architecture:'+nre.file_stem+' ...')\n",
    "\n",
    "# Like_Dir = '../code/likes/'\n",
    "\n",
    "# Run_MCMC = True\n",
    "# Run_NRE = True\n",
    "\n",
    "# Max_Samples = 1000000\n",
    "# Rminus1_Stop = 0.01 # 0.005\n",
    "# Rminus1_CL_Stop = 0.05 # 0.025\n",
    "# Rminus1_CL_Level = 0.95 # 95\n",
    "\n",
    "# Burn_In = 0\n",
    "# Burn_Frac = 0.3 # for later use\n",
    "\n",
    "# Latex_List = ['a_{{{0:d}}}'.format(p) for p in range(nre.nparam)]\n",
    "# Params_List = ['a{0:d}'.format(p) for p in range(nre.nparam)]\n",
    "\n",
    "# # Nd_ln_2pi = nre.ndata*np.log(2*np.pi)\n",
    "# # Cinv,detC = nre.svd_inv(nre.cov_mat)\n",
    "# # ln_det_Cd = np.log(detC)\n",
    "# # constant = -0.5*(Nd_ln_2pi + ln_det_Cd) \n",
    "\n",
    "\n",
    "# info = {}\n",
    "# info['params'] = {}\n",
    "\n",
    "# if nre.file_stem[:8] == 'gaussmix':\n",
    "#     info['theory'] = {'examplelikes.GaussMixTheory':\n",
    "#                       {'python_path':Like_Dir,\n",
    "#                        'X':nre.rv(nre.xvals),'ncomp':nre.ncomp}}\n",
    "# else:\n",
    "#     info['theory'] = {'examplelikes.PolyTheory':\n",
    "#                       {'python_path':Like_Dir,\n",
    "#                        'X':nre.rv(nre.xvals)}}\n",
    "# for p in range(len(Params_List)):\n",
    "#     ref = 0.0\n",
    "#     info['params'][Params_List[p]] = {'ref':{'min':ref-0.001,'max':ref+0.001},\n",
    "#                                       'prior':{'dist':'norm','loc':nre.prior_mean[p],'scale':nre.prior_std[p]},\n",
    "#                                       'proposal':0.01,'latex':Latex_List[p]}\n",
    "\n",
    "# info['sampler'] = {'mcmc':\n",
    "#                    {'learn_proposal': True,\n",
    "#                     'Rminus1_single_split': 4,\n",
    "#                     'measure_speeds': True,\n",
    "#                     'max_samples': Max_Samples,\n",
    "#                     'max_tries': 1000,\n",
    "#                     'Rminus1_stop': Rminus1_Stop,\n",
    "#                     'Rminus1_cl_stop': Rminus1_CL_Stop,\n",
    "#                     'Rminus1_cl_level': Rminus1_CL_Level,\n",
    "#                     'burn_in': Burn_In}}\n",
    "# info_output = File_Stem + '/stats/chains/'\n",
    "# chain_plot_path = info_output+'/for_plots'\n",
    "# Path(chain_plot_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# info[\"force\"] = True    \n",
    "# info[\"debug\"] = 40 # 40 = only error output generated (https://docs.python.org/2/library/logging.html#logging-levels)\n",
    "\n",
    "# info_nre = copy.deepcopy(info)\n",
    "# info_nre['likelihood'] = {'likelihoods.NRELike':\n",
    "#                          {'python_path':Like_Dir}}\n",
    "\n",
    "# Tasks = []\n",
    "# print('... setting up tasks')\n",
    "# for r in range(N_repeat):\n",
    "#     theta_test_this = nre.prior(1)\n",
    "#     data_test_this = nre.simulator(theta_test_this)\n",
    "\n",
    "#     info_r = copy.deepcopy(info)\n",
    "#     info_nre_r = copy.deepcopy(info_nre)\n",
    "#     nre_r = copy.deepcopy(nre)\n",
    "    \n",
    "#     info_r['output'] = info_output + 'mcmc_C{0:d}_r{1:d}'.format(Complexity,r+1)\n",
    "#     info_nre_r['output'] = info_output + 'nre_C{0:d}_r{1:d}'.format(Complexity,r+1)    \n",
    "#     info_r['likelihood'] = {'likelihoods.Chi2Like':\n",
    "#                           {'python_path':Like_Dir,\n",
    "#                            'X':nre.rv(nre.xvals),'Y':data_test_this.T,'cov_mat':nre.cov_mat}}\n",
    "    \n",
    "#     info_nre_r['theory'] = {'likelihoods.NRETheory':\n",
    "#                              {'python_path':Like_Dir,\n",
    "#                               'nre':nre_r,'data':data_test_this,'keys':Params_List}}\n",
    "#     run_mcmc = Run_MCMC if r==0 else False\n",
    "#     Tasks.append((nre_r,info_r,info_nre_r,run_mcmc,run_chains))\n",
    "\n",
    "#     del nre_r,info_r,info_nre_r\n",
    "#     gc.collect()\n",
    "    \n",
    "#     nre.status_bar(r,N_repeat)\n",
    "\n",
    "# def queue_chains(r,nre,info,info_nre,run_mcmc,run_chains,mdict):\n",
    "#     # p = psutil.Process()\n",
    "#     # p.cpu_affinity([n])\n",
    "#     KL = run_chains(nre,info,info_nre,run_mcmc)\n",
    "#     mdict[r+1] = KL\n",
    "#     # mdict[r+1] = {'KL':KL,'data':info_nre['likelihoods.NRETheory']['data']}\n",
    "#     return\n",
    "\n",
    "\n",
    "# def run_chains(nre,info,info_nre,run_mcmc):\n",
    "#     # Run chains\n",
    "#     if run_mcmc:\n",
    "#     # if Run_MCMC:\n",
    "#         updated_info, sampler = run(info)\n",
    "#     # else:\n",
    "#     #     print('Chains (hopefully) exist!')\n",
    "    \n",
    "#     if Run_NRE:\n",
    "#         updated_info_nre, sampler_nre = run(info_nre)\n",
    "#     # else:\n",
    "#     #     print('Chains (hopefully) exist!')\n",
    "#     # Load chains\n",
    "#     # input_path = str(Path(info[\"output\"]).resolve())\n",
    "#     input_path = str(Path(info_nre[\"output\"]).resolve()) # note info_nre\n",
    "#     gd_sample = loadMCSamples(input_path,settings={'ignore_rows':Burn_Frac})\n",
    "#     sample = gd_sample.samples.T\n",
    "    \n",
    "#     # Subsample chains\n",
    "#     N_Boot = np.min([10000,int(0.2*sample[0].size)])\n",
    "#     Ind = gd_sample.random_single_samples_indices(random_state=42,max_samples=N_Boot)\n",
    "#     N_Boot = Ind.size\n",
    "    \n",
    "#     # KL divergence D_{KL}(post_NRE || post_MCMC)\n",
    "#     data_this = info_nre['theory']['likelihoods.NRETheory']['data']\n",
    "#     KL = 0.0\n",
    "#     for b in range(N_Boot):\n",
    "#         # lnPrior_b = -sample[-4,Ind[b]]\n",
    "#         theta_b = nre.cv(sample[:nre.nparam,Ind[b]])\n",
    "#         ratio,F,theta_hat = analytical_nre(nre,data_this,theta_b)\n",
    "#         ratio_NRE = nre.predict(data_this,theta_b)\n",
    "#         KL += np.log((ratio_NRE+1e-30)/(ratio+1e-30))\n",
    "#         # lnPostNRE_b = np.log(nre.predict(data_this,theta_b)) + lnPrior_b\n",
    "#         # # chi2_b = sample[-2,Ind[b]]\n",
    "#         # # lnPostMCMC_b = -0.5*chi2_b + lnPrior_b + constant\n",
    "#         # lnPostMCMC_b = np.log(ratio) + lnPrior_b\n",
    "#         # KL += lnPostMCMC_b - lnPostNRE_b\n",
    "#     KL /= (N_Boot*np.log(2))\n",
    "#     return KL\n",
    "\n",
    "# KLvals = np.zeros(N_repeat)\n",
    "# # Data = np.zeros((N_repeat,nre.ndata,1))\n",
    "# start_time = time()\n",
    "# KLDict = nre.run_processes(Tasks,queue_chains,NProc)\n",
    "# for r in range(N_repeat):\n",
    "#     KLvals[r] = KLDict[r+1]#['KL']\n",
    "#     # Data[r] = KLDict[r+1]['data']\n",
    "    \n",
    "# print('... cleanup')\n",
    "\n",
    "# if Run_MCMC:\n",
    "#     files = [f for f in Path().glob(info_output+'mcmc_C{0:d}_r1.*'.format(Complexity))]\n",
    "#     for f in files:\n",
    "#         shutil.copy(f,chain_plot_path)\n",
    "\n",
    "# if Run_NRE:\n",
    "#     files = [f for f in Path().glob(info_output+'nre_C{0:d}_r1.*'.format(Complexity))]\n",
    "#     for f in files:\n",
    "#         shutil.copy(f,chain_plot_path)\n",
    "\n",
    "# files = [f for f in Path().glob(info_output+'mcmc*.*')]\n",
    "# for f in files:\n",
    "#     Path.unlink(f)\n",
    "\n",
    "# files = [f for f in Path().glob(info_output+'nre_C*.*')]\n",
    "# for f in files:\n",
    "#     Path.unlink(f)\n",
    "\n",
    "# del Tasks\n",
    "# gc.collect()\n",
    "\n",
    "# nre.time_this(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0db68-53cd-4a42-9b4e-ad3f11eae06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
